{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJUyRbgRpa6D"
      },
      "source": [
        "**🔹 Import Libraries and Set Hyperparameters**\n",
        "\n",
        "- Imports the necessary libraries: `pennylane`, `math`, and Pennylane’s version of `numpy`.\n",
        "\n",
        "- Defines key configuration values:\n",
        "  - `n_qubits = 4`: Number of qubits used in each quantum circuit.\n",
        "  - `patch_size = 2`: Each image is divided into 2×2 patches.\n",
        "  - `layers = 1`: Number of trainable layers in the quantum circuit.\n",
        "  - `levels = 3`: Number of hierarchical levels (for future multi-layer structure).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_G1Zxm5ob6xB",
        "outputId": "475cde60-ed35-414a-d516-1d685e2ab660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting qiskit==1.4.2\n",
            "  Downloading qiskit-1.4.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.41.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting silence-tensorflow\n",
            "  Downloading silence_tensorflow-1.2.3.tar.gz (7.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rustworkx>=0.15.0 (from qiskit==1.4.2)\n",
            "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (1.15.3)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (1.13.1)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (0.3.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (2.9.0.post0)\n",
            "Collecting stevedore>=3.0.0 (from qiskit==1.4.2)\n",
            "  Downloading stevedore-5.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (4.13.2)\n",
            "Collecting symengine<0.14,>=0.11 (from qiskit==1.4.2)\n",
            "  Downloading symengine-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.5)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.8.0)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.2)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.41 (from pennylane)\n",
            "  Downloading pennylane_lightning-0.41.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.41->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.29.265.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Collecting pbr>=2.0.0 (from stevedore>=3.0.0->qiskit==1.4.2)\n",
            "  Downloading pbr-6.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.3->qiskit==1.4.2) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading qiskit-1.4.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane-0.41.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.1-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.41.1-cp311-cp311-manylinux_2_28_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.4.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symengine-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pbr-6.1.1-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.29.265.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: silence-tensorflow\n",
            "  Building wheel for silence-tensorflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for silence-tensorflow: filename=silence_tensorflow-1.2.3-py3-none-any.whl size=6749 sha256=9fa9fefe697ad30dcd6c15cb89d0e78a04be22f2904ae5bfad3e89c7da59ed7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/5f/7e/afa8e22bf573d8aa309e5c8aed0d1a327076c5df2e12f68612\n",
            "Successfully built silence-tensorflow\n",
            "Installing collected packages: silence-tensorflow, appdirs, symengine, scipy-openblas32, rustworkx, pbr, autoray, stevedore, diastatic-malt, qiskit, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.1 diastatic-malt-2.15.2 pbr-6.1.1 pennylane-0.41.1 pennylane-lightning-0.41.1 qiskit-1.4.2 rustworkx-0.16.0 scipy-openblas32-0.3.29.265.0 silence-tensorflow-1.2.3 stevedore-5.4.1 symengine-0.13.0\n"
          ]
        }
      ],
      "source": [
        "pip install qiskit==1.4.2 tensorflow numpy pandas matplotlib pennylane silence-tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_5FClr3cpkp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')\n",
        "from silence_tensorflow import silence_tensorflow\n",
        "silence_tensorflow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaHSXhY8ooiA"
      },
      "source": [
        "**🔹 Data Loading and Preprocessing**\n",
        "\n",
        "- Loads the **MNIST** dataset (handwritten digit images).\n",
        "\n",
        "- **Normalization**: Scales pixel values from **0–255** to **0–1** by dividing by 255.\n",
        "\n",
        "- Takes one training image (`train_images[0]`) and the first 10 test images (`test_images[0:10]`).\n",
        "\n",
        "- Converts all image arrays to `float32` type (for compatibility with TensorFlow and PennyLane).\n",
        "\n",
        "- Displays one training image using **matplotlib** to verify preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llg6oYEBcu78",
        "outputId": "256f3172-505d-4640-9f84-c58df88b4fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "img shape: (28, 28)\n",
            "label: 5\n",
            "test_images shape: (10, 28, 28)\n",
            "test_labels shape: (10,)\n"
          ]
        }
      ],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "# Preprocess training data\n",
        "img = train_images[0] / 255.0\n",
        "img = tf.cast(img, tf.float32)\n",
        "label = train_labels[0]\n",
        "# Preprocess test data\n",
        "test_images = test_images[0:10] / 255.0  # Also normalize test images\n",
        "test_images = tf.cast(test_images, tf.float32)\n",
        "test_labels = test_labels[0:10]\n",
        "print(\"img shape:\", img.shape)\n",
        "print(\"label:\", label)\n",
        "print(\"test_images shape:\", test_images.shape)\n",
        "print(\"test_labels shape:\", test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zjMhGEpP37"
      },
      "source": [
        "**🔹 Visualizing the Image**\n",
        "\n",
        "- Plots the selected training image using `matplotlib`.\n",
        "\n",
        "- Applies a grayscale colormap (`gray`) for display.\n",
        "\n",
        "- Displays the digit label as the plot title.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "1_mwftwgcvzb",
        "outputId": "44929b05-42a6-4f82-a494-91d0594e2fd8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the input image\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8V-uC86c2l2",
        "outputId": "b74dd5ff-a7c2-40e7-bb32-16ae278e746f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pennylane/capture/capture_operators.py:33: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.4.28. You have version 0.5.2 installed. Please downgrade JAX to <=0.4.28 to avoid runtime errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwqpY4Rqc9uP"
      },
      "outputs": [],
      "source": [
        "n_qubits = 4\n",
        "patch_size = 2\n",
        "layers = 1\n",
        "levels=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHZxYrJlphA6"
      },
      "source": [
        "**🔹 Quantum Circuit Definition**\n",
        "\n",
        "- A quantum circuit is built using PennyLane and integrated with TensorFlow.\n",
        "\n",
        "- The circuit structure:\n",
        "  - **Feature map**: Applies `RY` rotations using input data to encode classical information.\n",
        "  - **Variational layer**:\n",
        "    - Applies trainable `RY` rotations for each qubit.\n",
        "    - Adds entanglement between neighboring qubits using `CNOT` gates.\n",
        "  - **Measurement**: Measures the expectation value of `PauliZ` on each qubit.\n",
        "\n",
        "- The circuit returns a list of `n_qubits` expectation values for further use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FctMPaGc_G5"
      },
      "outputs": [],
      "source": [
        "dev = qml.device('default.qubit', wires=n_qubits)\n",
        "@qml.qnode(dev, interface=\"tf\")\n",
        "@tf.autograph.experimental.do_not_convert\n",
        "def quantum_circuit(inputs, weights):\n",
        "    for i in range(n_qubits):  # feature map\n",
        "        qml.RY(np.pi * inputs[i], wires=i)\n",
        "\n",
        "    for l in range(layers):\n",
        "        for i in range(n_qubits):\n",
        "            qml.RY(weights[l][i], wires=i)\n",
        "        for i in range(n_qubits):\n",
        "            qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
        "\n",
        "    # Return the list of expectation values directly — no tf.stack\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XayW4BvWpjb9"
      },
      "source": [
        "**🔹 Image Patch Extraction**\n",
        "\n",
        "- Defines `extract_patches()` function to divide the 28×28 image into non-overlapping patches.\n",
        "\n",
        "- Each patch is of size `patch_size × patch_size` (2×2).\n",
        "\n",
        "- Each 2×2 patch is flattened to a vector and stacked into a tensor.\n",
        "\n",
        "- This step is essential to map image patches to the fixed input size required by quantum circuits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruzrvcRqdPB5"
      },
      "outputs": [],
      "source": [
        "def extract_patches(img, patch_size):\n",
        "    patches = []\n",
        "    for i in range(0, img.shape[0], patch_size):\n",
        "        for j in range(0, img.shape[1], patch_size):\n",
        "            patch = img[i:i+patch_size, j:j+patch_size]\n",
        "            if patch.shape == (patch_size, patch_size):\n",
        "                patches.append(tf.reshape(patch, [-1]))\n",
        "    return tf.stack(patches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN6YkYkFpmCT"
      },
      "source": [
        "**🔹 Initialize Trainable Quantum Weights**\n",
        "\n",
        "- Random seeds are set for reproducibility.\n",
        "\n",
        "- Three sets of weights (`weights_l1`, `weights_l2`, `weights_l3`) are initialized for three levels.\n",
        "\n",
        "- Each set has shape `(layers, n_qubits)` which matches the number of trainable gates per layer and per qubit.\n",
        "\n",
        "- Currently, only `weights_l1` is actively used for building the quantum layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coHoutJedFR2"
      },
      "outputs": [],
      "source": [
        "#all_weights = []\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "weights_l1 = tf.Variable(tf.random.normal((layers, n_qubits),stddev=0.1),trainable=True)# no of weights is (layers * n_qubits)\n",
        "weights_l2 = tf.Variable(tf.random.normal((layers, n_qubits),stddev=0.1),trainable=True)# no of weights is (layers * n_qubits)\n",
        "weights_l3 = tf.Variable(tf.random.normal((layers, n_qubits),stddev=0.1),trainable=True)# no of weights is (layers * n_qubits)\n",
        "print(\"weights in level 1 shape:\", weights_l1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xhyskQ3pohP"
      },
      "source": [
        "**🔹 Classical Neural Network Model**\n",
        "\n",
        "- A simple classical neural network is defined using `tf.keras.Sequential`.\n",
        "\n",
        "- Takes an input of shape `(9,)` – expected to be quantum outputs or processed features.\n",
        "\n",
        "- Applies a `Dense` layer with 10 output nodes using `softmax` activation.\n",
        "\n",
        "- Designed to classify digits (0–9) based on quantum-derived features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGs3KJmjNLQq"
      },
      "outputs": [],
      "source": [
        "# Classical model (trainable)\n",
        "classical_model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(9,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "print(\"combining with classical model\")\n",
        "\n",
        "# this is for classical parameteres defined for the classical part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnKGiJ8YqPXj"
      },
      "source": [
        "**🔹 Optimizer and Loss Function**\n",
        "\n",
        "- Defines the **Adam optimizer** with a learning rate of 0.01.\n",
        "\n",
        "- Uses **Sparse Categorical Crossentropy** as the loss function — suitable for multi-class classification where labels are integers (e.g., MNIST digits 0–9).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CInd0xY0NUGB"
      },
      "outputs": [],
      "source": [
        "# Optimizer and loss\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV6sOdBuNYaS"
      },
      "source": [
        "concatenating quantum part together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRl1NHBbqRQD"
      },
      "source": [
        "**🔹 Forward Pass (Quantum + Classical Integration)**\n",
        "\n",
        "- **Level 1**:\n",
        "  - Extracts 2×2 patches from the 28×28 image → gets 196 patches.\n",
        "  - Each patch is processed by the quantum circuit with `weights_l1`.\n",
        "  - Output from 4 qubits is averaged into a single value per patch.\n",
        "  - All 196 values are reshaped back to a 14×14 matrix (like an intermediate feature map).\n",
        "\n",
        "- **Level 2**:\n",
        "  - The 14×14 output is again patched (2×2) → results in 49 patches.\n",
        "  - Each patch is passed through the quantum circuit with `weights_l2`.\n",
        "  - Again, outputs are averaged and reshaped into a 7×7 image.\n",
        "\n",
        "- **Level 3**:\n",
        "  - The 7×7 output is patched into 9 patches.\n",
        "  - These are passed through the quantum circuit using `weights_l3`.\n",
        "  - Outputs are reduced to 9 values → final feature vector of shape (1, 9).\n",
        "\n",
        "- The 9-dimensional quantum-processed feature vector is fed into the **classical model** for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56H8fc7Rdggd"
      },
      "outputs": [],
      "source": [
        "def forward(x_img):\n",
        "    # First layer: Extract patches from original image (28x28 -> 14x14 patches -> 196 patches)\n",
        "    patches1 = extract_patches(x_img, patch_size)  # Shape: (196, 4)\n",
        "    print(\"patches1 after extracting:\", patches1.shape)\n",
        "\n",
        "    # Process first layer patches\n",
        "    q_outputs1 = []\n",
        "    for i in range(patches1.shape[0]):\n",
        "        patch = patches1[i]\n",
        "        q_output1 = quantum_circuit(patch, weights_l1)  # Returns list of 4 values\n",
        "        if isinstance(q_output1, (list, tuple)):\n",
        "            q_output1 = tf.stack(q_output1)  # Convert list to tensor\n",
        "        # Take real part to avoid complex number warnings\n",
        "        q_output1 = tf.cast(tf.math.real(q_output1), tf.float32)\n",
        "        # Take average of the 4 quantum features\n",
        "        q_output_1_avg = tf.reduce_mean(q_output1)  # Average to single value\n",
        "        q_outputs1.append(q_output_1_avg)\n",
        "\n",
        "    q_outputs1 = tf.stack(q_outputs1)  # Shape: (196,) - one value per patch\n",
        "    q_outputs1_reshaped = tf.reshape(q_outputs1, (14, 14))  # Reshape back to 14x14 image\n",
        "    print(\"q_outputs1 shape after reshaping to image:\", q_outputs1_reshaped.shape)\n",
        "\n",
        "    # Second layer: Extract patches from the 14x14 quantum output (14x14 -> 7x7 patches -> 49 patches)\n",
        "    patches2 = extract_patches(q_outputs1_reshaped, patch_size)  # Shape: (49, 4)\n",
        "    print(\"patches2 after extracting:\", patches2.shape)\n",
        "\n",
        "    # Process second layer patches\n",
        "    q_outputs2 = []\n",
        "    for i in range(patches2.shape[0]):\n",
        "        patch = patches2[i]\n",
        "        q_output2 = quantum_circuit(patch, weights_l2)  # Returns list of 4 values\n",
        "        if isinstance(q_output2, (list, tuple)):\n",
        "            q_output2 = tf.stack(q_output2)  # Convert list to tensor\n",
        "        # Take real part to avoid complex number warnings\n",
        "        q_output2 = tf.cast(tf.math.real(q_output2), tf.float32)\n",
        "        # Take average of the 4 quantum features\n",
        "        q_output_2_avg = tf.reduce_mean(q_output2)  # Average to single value\n",
        "        q_outputs2.append(q_output_2_avg)\n",
        "\n",
        "    q_outputs2 = tf.stack(q_outputs2)  # Shape: (49,) - one value per patch\n",
        "    q_outputs2_reshaped = tf.reshape(q_outputs2, (7, 7))  # Reshape back to 7x7 image\n",
        "    print(\"q_outputs2 shape after reshaping to image:\", q_outputs2_reshaped.shape)\n",
        "\n",
        "    # Third layer: Extract patches from the 7x7 quantum output (7x7 -> 3x3 patches -> 9 patches)\n",
        "    patches3 = extract_patches(q_outputs2_reshaped, patch_size)  # Shape: (9, 4)\n",
        "    print(\"patches3 after extracting:\", patches3.shape)\n",
        "\n",
        "    # Process third layer patches\n",
        "    q_outputs3 = []\n",
        "    for i in range(patches3.shape[0]):\n",
        "        patch = patches3[i]\n",
        "        q_output3 = quantum_circuit(patch, weights_l3)  # Returns list of 4 values\n",
        "        if isinstance(q_output3, (list, tuple)):\n",
        "            q_output3 = tf.stack(q_output3)  # Convert list to tensor\n",
        "        # Take real part to avoid complex number warnings\n",
        "        q_output3 = tf.cast(tf.math.real(q_output3), tf.float32)\n",
        "        # Take average of the 4 quantum features\n",
        "        q_output_3_avg = tf.reduce_mean(q_output3)  # Average to single value\n",
        "        q_outputs3.append(q_output_3_avg)\n",
        "\n",
        "    q_outputs3 = tf.stack(q_outputs3)  # Shape: (9,) - one value per patch\n",
        "    q_outputs3_reshaped = tf.reshape(q_outputs3, (1, -1))  # Shape: (1, 9) - flattened\n",
        "    print(\"q_outputs3 shape:\", q_outputs3_reshaped.shape)\n",
        "\n",
        "    # Pass through classical model\n",
        "    logits = classical_model(q_outputs3_reshaped)\n",
        "    print(\"shape after sending the third quantum layer to classical model:\", logits.shape)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XORT6CZqUbi"
      },
      "source": [
        "**🔹 Training Step with Gradient Tape**\n",
        "\n",
        "- Wraps the forward pass using `tf.GradientTape()` to compute gradients.\n",
        "\n",
        "- Reshapes the label to match the model’s output.\n",
        "\n",
        "- Computes the loss using the predefined loss function.\n",
        "\n",
        "- Collects **trainable variables**: `weights_l1`, `weights_l2`, and classical model weights.\n",
        "\n",
        "- Calculates gradients and applies them using the Adam optimizer.\n",
        "\n",
        "- Returns both the loss and model predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CfB5BUEQHM2"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(x_img, y_label):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = forward(x_img)  # shape: (1, 10)\n",
        "        loss = loss_fn(tf.reshape(y_label, (1,)), predictions)\n",
        "        tf.print(\"Loss:\", loss)\n",
        "\n",
        "    trainable_vars = [weights_l1, weights_l2] + classical_model.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "    return loss, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onTBoNiGqW5y"
      },
      "source": [
        "**🔹 Training Loop (Single Image, 20 Epochs)**\n",
        "\n",
        "- Runs for 20 epochs on a single image and label.\n",
        "\n",
        "- In each epoch:\n",
        "  - Performs forward + backward pass using `train_step()`.\n",
        "  - Calculates and prints loss and accuracy.\n",
        "  - Logs detailed output:\n",
        "    - Prediction shape\n",
        "    - Prediction values\n",
        "    - Ground truth label\n",
        "    - Final predicted class\n",
        "\n",
        "- This loop helps track how the model evolves while training on a very small input set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pmQhG7tbR-P-",
        "outputId": "7914f186-a277-4845-9db2-bf7c7360b4ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "patches1 after extracting: (196, 4)\n",
            "q_outputs1 shape after reshaping to image: (14, 14)\n",
            "patches2 after extracting: (49, 4)\n",
            "q_outputs2 shape after reshaping to image: (7, 7)\n",
            "patches3 after extracting: (9, 4)\n",
            "q_outputs3 shape: (1, 9)\n",
            "shape after sending the third quantum layer to classical model: (1, 10)\n",
            "patches1 after extracting: (196, 4)\n",
            "q_outputs1 shape after reshaping to image: (14, 14)\n",
            "patches2 after extracting: (49, 4)\n",
            "q_outputs2 shape after reshaping to image: (7, 7)\n",
            "patches3 after extracting: (9, 4)\n",
            "q_outputs3 shape: (1, 9)\n",
            "shape after sending the third quantum layer to classical model: (1, 10)\n",
            "Loss: 3.02301121\n",
            "Epoch  1/20 | Loss: 3.023011 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 1:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.12188376 0.04397007 0.09475879 0.04848508 0.25418234 0.04865448\n",
            " 0.17412034 0.13260227 0.03500326 0.04633961]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 2.89250278\n",
            "Epoch  2/20 | Loss: 2.892503 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 2:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.12119228 0.04382496 0.09263524 0.04800531 0.2552217  0.05543729\n",
            " 0.17019346 0.13268733 0.03503029 0.04577216]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 2.76553035\n",
            "Epoch  3/20 | Loss: 2.765530 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 3:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.12022131 0.04363051 0.09052248 0.04747288 0.25608614 0.06294271\n",
            " 0.16622818 0.13274755 0.03501277 0.04513549]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 2.64217234\n",
            "Epoch  4/20 | Loss: 2.642172 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 4:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.1188653  0.04337254 0.0884757  0.04684352 0.2568153  0.07120642\n",
            " 0.16234384 0.13273636 0.03490333 0.04443767]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 2.52207899\n",
            "Epoch  5/20 | Loss: 2.522079 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 5:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.1169631  0.04304637 0.08659071 0.04605495 0.2574394  0.08029249\n",
            " 0.15871768 0.1325477  0.0346352  0.04371237]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 2.40438652\n",
            "Epoch  6/20 | Loss: 2.404387 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 6:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.11436877 0.04267083 0.08498517 0.04506132 0.25787804 0.09032089\n",
            " 0.15552595 0.13201337 0.03415364 0.04302207]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 2.28799176\n",
            "Epoch  7/20 | Loss: 2.287992 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 7:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.11106116 0.04225664 0.08369252 0.04385421 0.2579917  0.10147003\n",
            " 0.15279631 0.13103789 0.0334471  0.04239237]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 2.17210293\n",
            "Epoch  8/20 | Loss: 2.172103 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 8:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.10717565 0.04179556 0.08261989 0.04246491 0.25764593 0.11393777\n",
            " 0.15035199 0.12965888 0.03255809 0.04179132]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 2.05635786\n",
            "Epoch  9/20 | Loss: 2.056358 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 9:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.10287973 0.04127109 0.08163948 0.04093019 0.25673467 0.127919\n",
            " 0.1479693  0.12794657 0.03153842 0.04117155]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 1.94068241\n",
            "Epoch 10/20 | Loss: 1.940682 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 10:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.09830378 0.04066515 0.08064546 0.03927862 0.25516963 0.14360592\n",
            " 0.14547385 0.12593932 0.03042432 0.04049402]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 1.82519209\n",
            "Epoch 11/20 | Loss: 1.825192 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 11:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.09354082 0.03996003 0.07955725 0.03753333 0.25286782 0.16118667\n",
            " 0.14274177 0.12364493 0.02923851 0.03972887]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 1.71014833\n",
            "Epoch 12/20 | Loss: 1.710148 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 12:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.08865932 0.03913986 0.07831104 0.03571473 0.24974827 0.18083896\n",
            " 0.13968281 0.12105533 0.02799635 0.03885338]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 1.59592915\n",
            "Epoch 13/20 | Loss: 1.595929 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 13:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.08371215 0.03819156 0.07685412 0.0338414  0.24573527 0.20272009\n",
            " 0.13622865 0.11815657 0.02670958 0.03785057]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 1.48301327\n",
            "Epoch 14/20 | Loss: 1.483013 | Accuracy: 0.0000\n",
            "  Detailed info at epoch 14:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.07874238 0.03710568 0.07514259 0.03193099 0.24076469 0.22695278\n",
            " 0.13232896 0.11493472 0.02538828 0.0367089 ]\n",
            "    True label: 5\n",
            "    Predicted class: 4\n",
            "\n",
            "Loss: 1.37197161\n",
            "Epoch 15/20 | Loss: 1.371972 | Accuracy: 1.0000\n",
            "  Detailed info at epoch 15:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.07378718 0.03587712 0.07314225 0.03000083 0.23479016 0.25360644\n",
            " 0.12795162 0.11137978 0.02404197 0.03542263]\n",
            "    True label: 5\n",
            "    Predicted class: 5\n",
            "\n",
            "Loss: 1.26344621\n",
            "Epoch 16/20 | Loss: 1.263446 | Accuracy: 1.0000\n",
            "  Detailed info at epoch 16:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.06887995 0.03450594 0.07083053 0.02806793 0.22779109 0.28267816\n",
            " 0.12308476 0.10748957 0.02268007 0.03399196]\n",
            "    True label: 5\n",
            "    Predicted class: 5\n",
            "\n",
            "Loss: 1.15812647\n",
            "Epoch 17/20 | Loss: 1.158126 | Accuracy: 1.0000\n",
            "  Detailed info at epoch 17:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.06405227 0.03299806 0.0681993  0.02614908 0.21977893 0.31407404\n",
            " 0.11773932 0.10327285 0.02131239 0.03242378]\n",
            "    True label: 5\n",
            "    Predicted class: 5\n",
            "\n",
            "Loss: 1.05671155\n",
            "Epoch 18/20 | Loss: 1.056712 | Accuracy: 1.0000\n",
            "  Detailed info at epoch 18:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.05933409 0.03136564 0.06525685 0.0242604  0.21080345 0.34759697\n",
            " 0.11195066 0.09875131 0.01994901 0.03073165]\n",
            "    True label: 5\n",
            "    Predicted class: 5\n",
            "\n",
            "Loss: 0.959870636\n",
            "Epoch 19/20 | Loss: 0.959871 | Accuracy: 1.0000\n",
            "  Detailed info at epoch 19:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.05475411 0.02962707 0.06202895 0.02241699 0.20095558 0.3829424\n",
            " 0.10577842 0.09396046 0.01860019 0.02893582]\n",
            "    True label: 5\n",
            "    Predicted class: 5\n",
            "\n",
            "Loss: 0.868202\n",
            "Epoch 20/20 | Loss: 0.868202 | Accuracy: 1.0000\n",
            "  Detailed info at epoch 20:\n",
            "    Predictions shape: (1, 10)\n",
            "    Predictions: [0.05033898 0.02780638 0.05855834 0.02063268 0.19036615 0.4197055\n",
            " 0.09930459 0.08894899 0.01727613 0.02706226]\n",
            "    True label: 5\n",
            "    Predicted class: 5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    loss, predictions = train_step(img, tf.constant(label))\n",
        "    acc = tf.keras.metrics.sparse_categorical_accuracy(tf.reshape(tf.constant([label]), (1,)), predictions)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:2d}/{epochs} | Loss: {loss.numpy():.6f} | Accuracy: {acc.numpy()[0]:.4f}\")\n",
        "    print(f\"  Detailed info at epoch {epoch+1}:\")\n",
        "    print(f\"    Predictions shape: {predictions.shape}\")\n",
        "    print(f\"    Predictions: {predictions.numpy().flatten()}\")\n",
        "    print(f\"    True label: {label}\")\n",
        "    print(f\"    Predicted class: {tf.argmax(predictions, axis=1).numpy()[0]}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kBTWUYpip_sU",
        "outputId": "de46191a-f74a-48a9-d188-293c24d7b30a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing model performance...\n",
            "Testing on 10 samples...\n",
            "--------------------------------------------------\n",
            "patches1 after extracting: (196, 4)\n",
            "q_outputs1 shape after reshaping to image: (14, 14)\n",
            "patches2 after extracting: (49, 4)\n",
            "q_outputs2 shape after reshaping to image: (7, 7)\n",
            "patches3 after extracting: (9, 4)\n",
            "q_outputs3 shape: (1, 9)\n",
            "shape after sending the third quantum layer to classical model: (1, 10)\n",
            "Sample  1 | True: 7 | Pred: 5 | Correct: ✗ | Loss: 2.1111\n",
            "Sample  5 | True: 4 | Pred: 5 | Correct: ✗ | Loss: 2.0862\n",
            "Sample 10 | True: 9 | Pred: 7 | Correct: ✗ | Loss: 4.7237\n",
            "--------------------------------------------------\n",
            "FINAL RESULTS:\n",
            "Test Accuracy: 0.1000 (1/10)\n",
            "Average Loss: 2.967358\n",
            "--------------------------------------------------\n",
            "\n",
            "Class-wise Accuracy:\n",
            "------------------------------\n",
            "Class 0: 0.0000 (0/1)\n",
            "Class 1: 0.0000 (0/2)\n",
            "Class 2: 0.0000 (0/1)\n",
            "Class 4: 0.0000 (0/2)\n",
            "Class 5: 1.0000 (1/1)\n",
            "Class 7: 0.0000 (0/1)\n",
            "Class 9: 0.0000 (0/2)\n",
            "\n",
            "Most confused classes:\n",
            "True: 1 → Predicted: 5 (happened 2 times)\n",
            "True: 4 → Predicted: 5 (happened 2 times)\n",
            "True: 7 → Predicted: 5 (happened 1 times)\n",
            "True: 2 → Predicted: 5 (happened 1 times)\n",
            "True: 0 → Predicted: 5 (happened 1 times)\n"
          ]
        }
      ],
      "source": [
        "# Testing function\n",
        "@tf.function\n",
        "def test_step(x_img, y_label):\n",
        "    predictions = forward(x_img)  # shape: (1, 10)\n",
        "    loss = loss_fn(tf.reshape(y_label, (1,)), predictions)\n",
        "    predicted_class = tf.argmax(predictions, axis=1)[0]\n",
        "    return loss, predictions, predicted_class\n",
        "\n",
        "# Test on full test set\n",
        "def evaluate_model(x_test, y_test, num_samples=None):\n",
        "    \"\"\"\n",
        "    Evaluate the model on test data\n",
        "\n",
        "    Args:\n",
        "        x_test: Test images (tensor with shape like (10, 28, 28))\n",
        "        y_test: Test labels (tensor with shape like (10,))\n",
        "        num_samples: Number of samples to test (None for all)\n",
        "    \"\"\"\n",
        "    if num_samples is None:\n",
        "        num_samples = x_test.shape[0]\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "\n",
        "    print(f\"Testing on {num_samples} samples...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        test_img = x_test[i]  # Shape: (28, 28)\n",
        "        test_label = y_test[i]\n",
        "\n",
        "        # Test the model\n",
        "        loss, predictions, predicted_class = test_step(test_img, tf.constant(test_label))\n",
        "\n",
        "        # Accumulate metrics\n",
        "        total_loss += loss.numpy()\n",
        "        is_correct = (predicted_class.numpy() == test_label)\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "\n",
        "        all_predictions.append(predicted_class.numpy())\n",
        "        all_true_labels.append(test_label)\n",
        "\n",
        "        # Print progress every 5 samples (since we only have 10)\n",
        "        if (i + 1) % 5 == 0 or i == 0:\n",
        "            print(f\"Sample {i+1:2d} | True: {test_label} | Pred: {predicted_class.numpy()} | \"\n",
        "                  f\"Correct: {'✓' if is_correct else '✗'} | Loss: {loss.numpy():.4f}\")\n",
        "\n",
        "    # Calculate final metrics\n",
        "    avg_loss = total_loss / num_samples\n",
        "    accuracy = correct_predictions / num_samples\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"FINAL RESULTS:\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f} ({correct_predictions}/{num_samples})\")\n",
        "    print(f\"Average Loss: {avg_loss:.6f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    return accuracy, avg_loss, all_predictions, all_true_labels\n",
        "\n",
        "# Example usage - test on your 10 samples\n",
        "print(\"Testing model performance...\")\n",
        "test_accuracy, test_loss, predictions, true_labels = evaluate_model(test_images, test_labels)\n",
        "\n",
        "# Detailed analysis\n",
        "def analyze_results(predictions, true_labels):\n",
        "    \"\"\"Analyze prediction results in detail\"\"\"\n",
        "    from collections import Counter\n",
        "\n",
        "    # Class-wise accuracy\n",
        "    class_correct = Counter()\n",
        "    class_total = Counter()\n",
        "\n",
        "    for pred, true in zip(predictions, true_labels):\n",
        "        class_total[true] += 1\n",
        "        if pred == true:\n",
        "            class_correct[true] += 1\n",
        "\n",
        "    print(\"\\nClass-wise Accuracy:\")\n",
        "    print(\"-\" * 30)\n",
        "    for class_id in sorted(class_total.keys()):\n",
        "        acc = class_correct[class_id] / class_total[class_id] if class_total[class_id] > 0 else 0\n",
        "        print(f\"Class {class_id}: {acc:.4f} ({class_correct[class_id]}/{class_total[class_id]})\")\n",
        "\n",
        "    # Confusion matrix (simplified)\n",
        "    print(f\"\\nMost confused classes:\")\n",
        "    errors = [(true, pred) for pred, true in zip(predictions, true_labels) if pred != true]\n",
        "    error_counts = Counter(errors)\n",
        "    for (true, pred), count in error_counts.most_common(5):\n",
        "        print(f\"True: {true} → Predicted: {pred} (happened {count} times)\")\n",
        "\n",
        "# Run detailed analysis\n",
        "analyze_results(predictions, true_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6wdgGEk-RTJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import numpy as np\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')\n",
        "from silence_tensorflow import silence_tensorflow\n",
        "silence_tensorflow()\n",
        "\n",
        "# Load MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Filter training data for classes 0 and 5 (30 samples each)\n",
        "def get_class_samples(images, labels, target_class, num_samples):\n",
        "    \"\"\"Get specified number of samples for a target class\"\"\"\n",
        "    class_indices = np.where(labels == target_class)[0]\n",
        "    selected_indices = class_indices[:num_samples]\n",
        "    return images[selected_indices], labels[selected_indices]\n",
        "\n",
        "# Get 30 samples each of class 0 and 5\n",
        "train_images_0, train_labels_0 = get_class_samples(train_images, train_labels, 0, 30)\n",
        "train_images_5, train_labels_5 = get_class_samples(train_images, train_labels, 5, 30)\n",
        "\n",
        "# Combine the training data\n",
        "train_images_selected = np.concatenate([train_images_0, train_images_5], axis=0)\n",
        "train_labels_selected = np.concatenate([train_labels_0, train_labels_5], axis=0)\n",
        "\n",
        "# Shuffle the training data\n",
        "shuffle_indices = np.random.permutation(len(train_images_selected))\n",
        "train_images_selected = train_images_selected[shuffle_indices]\n",
        "train_labels_selected = train_labels_selected[shuffle_indices]\n",
        "\n",
        "# Normalize training data\n",
        "train_images_selected = train_images_selected / 255.0\n",
        "train_images_selected = tf.cast(train_images_selected, tf.float32)\n",
        "\n",
        "# Prepare test data (100 samples from each class for comprehensive testing)\n",
        "def get_balanced_test_set(images, labels, samples_per_class=100):\n",
        "    \"\"\"Get balanced test set with specified samples per class\"\"\"\n",
        "    test_imgs = []\n",
        "    test_lbls = []\n",
        "\n",
        "    for class_id in range(10):  # MNIST has classes 0-9\n",
        "        class_indices = np.where(labels == class_id)[0]\n",
        "        selected_indices = class_indices[:samples_per_class]\n",
        "        test_imgs.append(images[selected_indices])\n",
        "        test_lbls.append(labels[selected_indices])\n",
        "\n",
        "    return np.concatenate(test_imgs), np.concatenate(test_lbls)\n",
        "\n",
        "test_images_balanced, test_labels_balanced = get_balanced_test_set(test_images, test_labels, 100)\n",
        "test_images_balanced = test_images_balanced / 255.0\n",
        "test_images_balanced = tf.cast(test_images_balanced, tf.float32)\n",
        "\n",
        "print(f\"Training data shape: {train_images_selected.shape}\")\n",
        "print(f\"Training labels shape: {train_labels_selected.shape}\")\n",
        "print(f\"Training class distribution: {np.bincount(train_labels_selected)}\")\n",
        "print(f\"Test data shape: {test_images_balanced.shape}\")\n",
        "print(f\"Test labels shape: {test_labels_balanced.shape}\")\n",
        "print(f\"Test class distribution: {np.bincount(test_labels_balanced)}\")\n",
        "\n",
        "# Quantum circuit setup\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp\n",
        "import math\n",
        "\n",
        "n_qubits = 4\n",
        "patch_size = 2\n",
        "layers = 1\n",
        "levels = 3\n",
        "\n",
        "dev = qml.device('default.qubit', wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"tf\")\n",
        "@tf.autograph.experimental.do_not_convert\n",
        "def quantum_circuit(inputs, weights):\n",
        "    for i in range(n_qubits):  # feature map\n",
        "        qml.RY(pnp.pi * inputs[i], wires=i)\n",
        "\n",
        "    for l in range(layers):\n",
        "        for i in range(n_qubits):\n",
        "            qml.RY(weights[l][i], wires=i)\n",
        "        for i in range(n_qubits):\n",
        "            qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
        "\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "def extract_patches(img, patch_size):\n",
        "    patches = []\n",
        "    for i in range(0, img.shape[0], patch_size):\n",
        "        for j in range(0, img.shape[1], patch_size):\n",
        "            patch = img[i:i+patch_size, j:j+patch_size]\n",
        "            if patch.shape == (patch_size, patch_size):\n",
        "                patches.append(tf.reshape(patch, [-1]))\n",
        "    return tf.stack(patches)\n",
        "\n",
        "# Initialize quantum weights\n",
        "tf.random.set_seed(42)\n",
        "pnp.random.seed(42)\n",
        "\n",
        "weights_l1 = tf.Variable(tf.random.normal((layers, n_qubits), stddev=0.1), trainable=True)\n",
        "weights_l2 = tf.Variable(tf.random.normal((layers, n_qubits), stddev=0.1), trainable=True)\n",
        "weights_l3 = tf.Variable(tf.random.normal((layers, n_qubits), stddev=0.1), trainable=True)\n",
        "\n",
        "# Classical model\n",
        "classical_model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(9,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Optimizer and loss\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "def forward(x_img):\n",
        "    # First layer\n",
        "    patches1 = extract_patches(x_img, patch_size)\n",
        "\n",
        "    q_outputs1 = []\n",
        "    for i in range(patches1.shape[0]):\n",
        "        patch = patches1[i]\n",
        "        q_output1 = quantum_circuit(patch, weights_l1)\n",
        "        if isinstance(q_output1, (list, tuple)):\n",
        "            q_output1 = tf.stack(q_output1)\n",
        "        q_output1 = tf.cast(tf.math.real(q_output1), tf.float32)\n",
        "        q_output_1_avg = tf.reduce_mean(q_output1)\n",
        "        q_outputs1.append(q_output_1_avg)\n",
        "\n",
        "    q_outputs1 = tf.stack(q_outputs1)\n",
        "    q_outputs1_reshaped = tf.reshape(q_outputs1, (14, 14))\n",
        "\n",
        "    # Second layer\n",
        "    patches2 = extract_patches(q_outputs1_reshaped, patch_size)\n",
        "\n",
        "    q_outputs2 = []\n",
        "    for i in range(patches2.shape[0]):\n",
        "        patch = patches2[i]\n",
        "        q_output2 = quantum_circuit(patch, weights_l2)\n",
        "        if isinstance(q_output2, (list, tuple)):\n",
        "            q_output2 = tf.stack(q_output2)\n",
        "        q_output2 = tf.cast(tf.math.real(q_output2), tf.float32)\n",
        "        q_output_2_avg = tf.reduce_mean(q_output2)\n",
        "        q_outputs2.append(q_output_2_avg)\n",
        "\n",
        "    q_outputs2 = tf.stack(q_outputs2)\n",
        "    q_outputs2_reshaped = tf.reshape(q_outputs2, (7, 7))\n",
        "\n",
        "    # Third layer\n",
        "    patches3 = extract_patches(q_outputs2_reshaped, patch_size)\n",
        "\n",
        "    q_outputs3 = []\n",
        "    for i in range(patches3.shape[0]):\n",
        "        patch = patches3[i]\n",
        "        q_output3 = quantum_circuit(patch, weights_l3)\n",
        "        if isinstance(q_output3, (list, tuple)):\n",
        "            q_output3 = tf.stack(q_output3)\n",
        "        q_output3 = tf.cast(tf.math.real(q_output3), tf.float32)\n",
        "        q_output_3_avg = tf.reduce_mean(q_output3)\n",
        "        q_outputs3.append(q_output_3_avg)\n",
        "\n",
        "    q_outputs3 = tf.stack(q_outputs3)\n",
        "    q_outputs3_reshaped = tf.reshape(q_outputs3, (1, -1))\n",
        "\n",
        "    # Classical layer\n",
        "    logits = classical_model(q_outputs3_reshaped)\n",
        "    return logits\n",
        "\n",
        "@tf.function\n",
        "def train_step(x_img, y_label):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = forward(x_img)\n",
        "        loss = loss_fn(tf.reshape(y_label, (1,)), predictions)\n",
        "\n",
        "    trainable_vars = [weights_l1, weights_l2, weights_l3] + classical_model.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "    return loss, predictions\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting training on 60 images (30 each of class 0 and 5)...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "epochs = 10\n",
        "batch_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # Shuffle training data each epoch\n",
        "    shuffle_indices = np.random.permutation(len(train_images_selected))\n",
        "    shuffled_images = tf.gather(train_images_selected, shuffle_indices)\n",
        "    shuffled_labels = tf.gather(train_labels_selected, shuffle_indices)\n",
        "\n",
        "    for i in range(len(shuffled_images)):\n",
        "        img = shuffled_images[i]\n",
        "        label = shuffled_labels[i]\n",
        "\n",
        "        loss, predictions = train_step(img, tf.constant(label))\n",
        "        epoch_loss += loss.numpy()\n",
        "\n",
        "        predicted_class = tf.argmax(predictions, axis=1).numpy()[0]\n",
        "        if predicted_class == label:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    avg_loss = epoch_loss / len(shuffled_images)\n",
        "    accuracy = correct_predictions / len(shuffled_images)\n",
        "    batch_losses.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:2d}/{epochs} | Loss: {avg_loss:.6f} | Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Testing function\n",
        "@tf.function\n",
        "def test_step(x_img, y_label):\n",
        "    predictions = forward(x_img)\n",
        "    loss = loss_fn(tf.reshape(y_label, (1,)), predictions)\n",
        "    predicted_class = tf.argmax(predictions, axis=1)[0]\n",
        "    return loss, predictions, predicted_class\n",
        "\n",
        "def evaluate_model(x_test, y_test, num_samples=None):\n",
        "    \"\"\"Evaluate model on test data\"\"\"\n",
        "    if num_samples is None:\n",
        "        num_samples = x_test.shape[0]\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    all_predictions = []\n",
        "    all_true_labels = []\n",
        "    class_correct = {}\n",
        "    class_total = {}\n",
        "\n",
        "    print(f\"Testing on {num_samples} samples...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        test_img = x_test[i]\n",
        "        test_label = y_test[i].numpy() if hasattr(y_test[i], 'numpy') else y_test[i]\n",
        "\n",
        "        loss, predictions, predicted_class = test_step(test_img, tf.constant(test_label))\n",
        "\n",
        "        total_loss += loss.numpy()\n",
        "        is_correct = (predicted_class.numpy() == test_label)\n",
        "        if is_correct:\n",
        "            correct_predictions += 1\n",
        "\n",
        "        all_predictions.append(predicted_class.numpy())\n",
        "        all_true_labels.append(test_label)\n",
        "\n",
        "        # Track class-wise accuracy\n",
        "        if test_label not in class_total:\n",
        "            class_total[test_label] = 0\n",
        "            class_correct[test_label] = 0\n",
        "        class_total[test_label] += 1\n",
        "        if is_correct:\n",
        "            class_correct[test_label] += 1\n",
        "\n",
        "        # Print progress every 100 samples\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Processed {i+1}/{num_samples} samples...\")\n",
        "\n",
        "    avg_loss = total_loss / num_samples\n",
        "    overall_accuracy = correct_predictions / num_samples\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"OVERALL RESULTS:\")\n",
        "    print(f\"Test Accuracy: {overall_accuracy:.4f} ({correct_predictions}/{num_samples})\")\n",
        "    print(f\"Average Loss: {avg_loss:.6f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Class-wise results\n",
        "    print(\"\\nCLASS-WISE ACCURACY:\")\n",
        "    print(\"-\" * 30)\n",
        "    for class_id in sorted(class_total.keys()):\n",
        "        acc = class_correct[class_id] / class_total[class_id] if class_total[class_id] > 0 else 0\n",
        "        print(f\"Class {class_id}: {acc:.4f} ({class_correct[class_id]}/{class_total[class_id]})\")\n",
        "\n",
        "    return overall_accuracy, avg_loss, all_predictions, all_true_labels\n",
        "\n",
        "# Test the model\n",
        "print(\"Testing model performance on all classes...\")\n",
        "test_accuracy, test_loss, predictions, true_labels = evaluate_model(test_images_balanced, test_labels_balanced)\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(batch_losses)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot confusion matrix visualization\n",
        "plt.subplot(1, 2, 2)\n",
        "from collections import Counter\n",
        "confusion_data = Counter([(true, pred) for true, pred in zip(true_labels, predictions)])\n",
        "\n",
        "# Create confusion matrix\n",
        "confusion_matrix = np.zeros((10, 10))\n",
        "for (true_label, pred_label), count in confusion_data.items():\n",
        "    confusion_matrix[true_label, pred_label] = count\n",
        "\n",
        "plt.imshow(confusion_matrix, cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.ylabel('True Class')\n",
        "plt.colorbar()\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        plt.text(j, i, int(confusion_matrix[i, j]),\n",
        "                ha=\"center\", va=\"center\", color=\"red\" if confusion_matrix[i, j] > 50 else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAnalysis Summary:\")\n",
        "print(\"- The model was trained on 60 images (30 each of class 0 and 5)\")\n",
        "print(\"- Tested on 1000 images (100 from each of the 10 classes)\")\n",
        "print(\"- Classes 0 and 5 should show better performance since they were in training\")\n",
        "print(\"- Other classes rely on the model's ability to generalize\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}