{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ”¹ Import Libraries and Set Hyperparameters**\n",
        "\n",
        "- Imports the necessary libraries: `pennylane`, `math`, and Pennylaneâ€™s version of `numpy`.\n",
        "\n",
        "- Defines key configuration values:\n",
        "  - `n_qubits = 4`: Number of qubits used in each quantum circuit.\n",
        "  - `patch_size = 2`: Each image is divided into 2Ã—2 patches.\n",
        "  - `layers = 1`: Number of trainable layers in the quantum circuit.\n",
        "  - `levels = 3`: Number of hierarchical levels (for future multi-layer structure).\n"
      ],
      "metadata": {
        "id": "jJUyRbgRpa6D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G1Zxm5ob6xB",
        "outputId": "6ffe874c-e0dd-4b7a-8bfd-a159fdfb214e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qiskit==1.4.2 in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.11/dist-packages (0.41.1)\n",
            "Requirement already satisfied: silence-tensorflow in /usr/local/lib/python3.11/dist-packages (1.2.3)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (0.16.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (1.15.3)\n",
            "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (1.13.1)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (0.3.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (2.9.0.post0)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (4.13.2)\n",
            "Requirement already satisfied: symengine<0.14,>=0.11 in /usr/local/lib/python3.11/dist-packages (from qiskit==1.4.2) (0.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.4.2)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.8.0)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray>=0.6.11 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.7.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: pennylane-lightning>=0.41 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.41.1)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning>=0.41->pennylane) (0.3.29.265.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: pbr>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from stevedore>=3.0.0->qiskit==1.4.2) (6.1.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.3->qiskit==1.4.2) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install qiskit==1.4.2 tensorflow numpy pandas matplotlib pennylane silence-tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_5FClr3cpkp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1979feed-4c3c-4700-d740-c3c40121264e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pennylane/capture/capture_operators.py:33: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.4.28. You have version 0.5.2 installed. Please downgrade JAX to <=0.4.28 to avoid runtime errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')\n",
        "from silence_tensorflow import silence_tensorflow\n",
        "silence_tensorflow()\n",
        "import pennylane as qml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ”¹ Data Loading and Preprocessing**\n",
        "\n",
        "- Loads the **MNIST** dataset (handwritten digit images).\n",
        "\n",
        "- **Normalization**: Scales pixel values from **0â€“255** to **0â€“1** by dividing by 255.\n",
        "\n",
        "- Takes one training image (`train_images[0]`) and the first 10 test images (`test_images[0:10]`).\n",
        "\n",
        "- Converts all image arrays to `float32` type (for compatibility with TensorFlow and PennyLane).\n",
        "\n",
        "- Displays one training image using **matplotlib** to verify preprocessing.\n"
      ],
      "metadata": {
        "id": "xaHSXhY8ooiA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llg6oYEBcu78",
        "outputId": "cd8def02-7813-425a-af93-10a66151872b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original x_train shape: (60000, 28, 28)\n",
            "Original y_train shape: (60000,)\n",
            "Original x_test shape: (10000, 28, 28)\n",
            "Original y_test shape: (10000,)\n",
            "Filtered x_train shape (only 0 & 5): (11344, 28, 28)\n",
            "Filtered y_train unique labels: [0 5]\n",
            "Filtered x_test shape (only 0 & 5): (1872, 28, 28)\n",
            "Filtered y_test unique labels: [0 5]\n",
            "Balanced x_train shape: (100, 28, 28)\n",
            "Balanced y_train distribution: [50 50]\n"
          ]
        }
      ],
      "source": [
        "# Load MNIST\n",
        "import numpy as np\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(f\"Original x_train shape: {x_train.shape}\")\n",
        "print(f\"Original y_train shape: {y_train.shape}\")\n",
        "print(f\"Original x_test shape: {x_test.shape}\")\n",
        "print(f\"Original y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Filter only digits 0 and 5\n",
        "train_filter = np.where((y_train == 0) | (y_train == 5))\n",
        "test_filter = np.where((y_test == 0) | (y_test == 5))\n",
        "\n",
        "x_train, y_train = x_train[train_filter], y_train[train_filter]\n",
        "x_test, y_test = x_test[test_filter], y_test[test_filter]\n",
        "\n",
        "print(f\"Filtered x_train shape (only 0 & 5): {x_train.shape}\")\n",
        "print(f\"Filtered y_train unique labels: {np.unique(y_train)}\")\n",
        "print(f\"Filtered x_test shape (only 0 & 5): {x_test.shape}\")\n",
        "print(f\"Filtered y_test unique labels: {np.unique(y_test)}\")\n",
        "\n",
        "# Relabel: 0 stays 0, 5 becomes 1\n",
        "y_train = np.where(y_train == 0, 0, 1)\n",
        "y_test = np.where(y_test == 0, 0, 1)\n",
        "\n",
        "# Normalize images and cast to float32\n",
        "x_train = (x_train / 255.0).astype(np.float32)\n",
        "x_test = (x_test / 255.0).astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "# Select 50 samples of each class (0 and 1) for training\n",
        "idx_0 = np.where(y_train == 0)[0][:50]\n",
        "idx_1 = np.where(y_train == 1)[0][:50]\n",
        "selected_idx = np.concatenate([idx_0, idx_1])\n",
        "\n",
        "x_train = x_train[selected_idx]\n",
        "y_train = y_train[selected_idx]\n",
        "\n",
        "print(f\"Balanced x_train shape: {x_train.shape}\")\n",
        "print(f\"Balanced y_train distribution: {np.bincount(y_train.astype(int))}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16 # choose based on your GPU/CPU capacity\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "LtQGUXbA1WKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ”¹ Visualizing the Image**\n",
        "\n",
        "- Plots the selected training image using `matplotlib`.\n",
        "\n",
        "- Applies a grayscale colormap (`gray`) for display.\n",
        "\n",
        "- Displays the digit label as the plot title.\n"
      ],
      "metadata": {
        "id": "W0zjMhGEpP37"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "1_mwftwgcvzb",
        "outputId": "afc1f2a2-b85b-4fb9-d409-fa67d3c4b082"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Pick an index to visualize\n",
        "index = 0\n",
        "# Reshape the image back to 28x28 for plotting\n",
        "image = x_train[index].reshape(28, 28)\n",
        "# Plot the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Label: {int(y_train[index])}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8V-uC86c2l2"
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwqpY4Rqc9uP"
      },
      "outputs": [],
      "source": [
        "n_qubits = 4\n",
        "patch_size = 2\n",
        "layers = 1\n",
        "levels=3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ”¹ Quantum Circuit Definition**\n",
        "\n",
        "- A quantum circuit is built using PennyLane and integrated with TensorFlow.\n",
        "\n",
        "- The circuit structure:\n",
        "  - **Feature map**: Applies `RY` rotations using input data to encode classical information.\n",
        "  - **Variational layer**:\n",
        "    - Applies trainable `RY` rotations for each qubit.\n",
        "    - Adds entanglement between neighboring qubits using `CNOT` gates.\n",
        "  - **Measurement**: Measures the expectation value of `PauliZ` on each qubit.\n",
        "\n",
        "- The circuit returns a list of `n_qubits` expectation values for further use.\n"
      ],
      "metadata": {
        "id": "UHZxYrJlphA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FctMPaGc_G5"
      },
      "outputs": [],
      "source": [
        "dev = qml.device('default.qubit', wires=n_qubits)\n",
        "@qml.qnode(dev, interface=\"tf\")\n",
        "@tf.autograph.experimental.do_not_convert\n",
        "def quantum_circuit(inputs, weights):\n",
        "    for i in range(n_qubits):  # feature map\n",
        "        qml.RY(np.pi * inputs[i], wires=i)\n",
        "\n",
        "    for l in range(layers):\n",
        "        for i in range(n_qubits):\n",
        "            qml.RY(weights[l][i], wires=i)\n",
        "        for i in range(n_qubits):\n",
        "            qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
        "\n",
        "    # Return the list of expectation values directly â€” no tf.stack\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ”¹ Image Patch Extraction**\n",
        "\n",
        "- Defines `extract_patches()` function to divide the 28Ã—28 image into non-overlapping patches.\n",
        "\n",
        "- Each patch is of size `patch_size Ã— patch_size` (2Ã—2).\n",
        "\n",
        "- Each 2Ã—2 patch is flattened to a vector and stacked into a tensor.\n",
        "\n",
        "- This step is essential to map image patches to the fixed input size required by quantum circuits.\n"
      ],
      "metadata": {
        "id": "XayW4BvWpjb9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruzrvcRqdPB5"
      },
      "outputs": [],
      "source": [
        "def extract_patches_batch(img_batch, patch_size):\n",
        "    # img_batch shape: (batch_size, H, W)\n",
        "    batch_size = img_batch.shape[0]\n",
        "    patches_all = []\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        patches = []\n",
        "        img = img_batch[b]\n",
        "        for i in range(0, img.shape[0], patch_size):\n",
        "            for j in range(0, img.shape[1], patch_size):\n",
        "                patch = img[i:i+patch_size, j:j+patch_size]\n",
        "                if patch.shape == (patch_size, patch_size):\n",
        "                    patches.append(tf.reshape(patch, [-1]))\n",
        "        patches_all.append(tf.stack(patches))  # Shape (num_patches, patch_dim)\n",
        "\n",
        "    return tf.stack(patches_all)  # Shape: (batch_size, num_patches, patch_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ”¹ Initialize Trainable Quantum Weights**\n",
        "\n",
        "- Random seeds are set for reproducibility.\n",
        "\n",
        "- Three sets of weights (`weights_l1`, `weights_l2`, `weights_l3`) are initialized for three levels.\n",
        "\n",
        "- Each set has shape `(layers, n_qubits)` which matches the number of trainable gates per layer and per qubit.\n",
        "\n",
        "- Currently, only `weights_l1` is actively used for building the quantum layer.\n"
      ],
      "metadata": {
        "id": "XN6YkYkFpmCT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coHoutJedFR2",
        "outputId": "23da95a8-9120-4fc4-fa03-1a57b465a5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights in level 1 shape: (1, 4)\n"
          ]
        }
      ],
      "source": [
        "#all_weights = []\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "weights_l1 = tf.Variable(tf.random.normal((layers, n_qubits),stddev=0.1),trainable=True)# no of weights is (layers * n_qubits)\n",
        "weights_l2 = tf.Variable(tf.random.normal((layers, n_qubits),stddev=0.1),trainable=True)# no of weights is (layers * n_qubits)\n",
        "weights_l3 = tf.Variable(tf.random.normal((layers, n_qubits),stddev=0.1),trainable=True)# no of weights is (layers * n_qubits)\n",
        "print(\"weights in level 1 shape:\", weights_l1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ”¹ Classical Neural Network Model**\n",
        "\n",
        "- A simple classical neural network is defined using `tf.keras.Sequential`.\n",
        "\n",
        "- Takes an input of shape `(9,)` â€“ expected to be quantum outputs or processed features.\n",
        "\n",
        "- Applies a `Dense` layer with 10 output nodes using `softmax` activation.\n",
        "\n",
        "- Designed to classify digits (0â€“9) based on quantum-derived features.\n"
      ],
      "metadata": {
        "id": "6xhyskQ3pohP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGs3KJmjNLQq",
        "outputId": "f3c86470-d759-4423-a993-8e06bea0ecdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combining with classical model\n"
          ]
        }
      ],
      "source": [
        "# Classical model (trainable)\n",
        "classical_model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(9,)),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "print(\"combining with classical model\")\n",
        "\n",
        "# this is for classical parameteres defined for the classical part"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ”¹ Optimizer and Loss Function**\n",
        "\n",
        "- Defines the **Adam optimizer** with a learning rate of 0.01.\n",
        "\n",
        "- Uses **Sparse Categorical Crossentropy** as the loss function â€” suitable for multi-class classification where labels are integers (e.g., MNIST digits 0 and 5).\n"
      ],
      "metadata": {
        "id": "fnKGiJ8YqPXj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CInd0xY0NUGB"
      },
      "outputs": [],
      "source": [
        "# Optimizer and loss\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV6sOdBuNYaS"
      },
      "source": [
        "concatenating quantum part together"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ”¹ Forward Pass (Quantum + Classical Integration)**\n",
        "\n",
        "- **Level 1**:\n",
        "  - Extracts 2Ã—2 patches from the 28Ã—28 image â†’ gets 196 patches.\n",
        "  - Each patch is processed by the quantum circuit with `weights_l1`.\n",
        "  - Output from 4 qubits is averaged into a single value per patch.\n",
        "  - All 196 values are reshaped back to a 14Ã—14 matrix (like an intermediate feature map).\n",
        "\n",
        "- **Level 2**:\n",
        "  - The 14Ã—14 output is again patched (2Ã—2) â†’ results in 49 patches.\n",
        "  - Each patch is passed through the quantum circuit with `weights_l2`.\n",
        "  - Again, outputs are averaged and reshaped into a 7Ã—7 image.\n",
        "\n",
        "- **Level 3**:\n",
        "  - The 7Ã—7 output is patched into 9 patches.\n",
        "  - These are passed through the quantum circuit using `weights_l3`.\n",
        "  - Outputs are reduced to 9 values â†’ final feature vector of shape (1, 9).\n",
        "\n",
        "- The 9-dimensional quantum-processed feature vector is fed into the **classical model** for classification.\n"
      ],
      "metadata": {
        "id": "hRl1NHBbqRQD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56H8fc7Rdggd"
      },
      "outputs": [],
      "source": [
        "def forward_batch(x_imgs):\n",
        "    batch_size = x_imgs.shape[0]\n",
        "\n",
        "    # First layer patches: (batch_size, num_patches, patch_dim)\n",
        "    patches1_batch = extract_patches_batch(x_imgs, patch_size)  # e.g., (batch, 196, 4)\n",
        "    print(\"patches1_batch shape:\", patches1_batch.shape)\n",
        "\n",
        "    q_outputs1_batch = []\n",
        "    for b in range(batch_size):\n",
        "        q_outputs1 = []\n",
        "        for i in range(patches1_batch.shape[1]):  # num_patches per image\n",
        "            patch = patches1_batch[b, i]\n",
        "            q_out = quantum_circuit(patch, weights_l1)\n",
        "            q_out = tf.stack(q_out) if isinstance(q_out, (list, tuple)) else q_out\n",
        "            q_out = tf.cast(tf.math.real(q_out), tf.float32)\n",
        "            q_outputs1.append(tf.reduce_mean(q_out))\n",
        "        q_outputs1 = tf.stack(q_outputs1)\n",
        "        q_outputs1_reshaped = tf.reshape(q_outputs1, (14, 14))\n",
        "        q_outputs1_batch.append(q_outputs1_reshaped)\n",
        "\n",
        "    q_outputs1_batch = tf.stack(q_outputs1_batch)  # (batch_size, 14, 14)\n",
        "    print(\"q_outputs1_batch shape:\", q_outputs1_batch.shape)\n",
        "\n",
        "    # Second layer patches\n",
        "    patches2_batch = extract_patches_batch(q_outputs1_batch, patch_size)  # (batch_size, 49, 4)\n",
        "    print(\"patches2_batch shape:\", patches2_batch.shape)\n",
        "\n",
        "    q_outputs2_batch = []\n",
        "    for b in range(batch_size):\n",
        "        q_outputs2 = []\n",
        "        for i in range(patches2_batch.shape[1]):\n",
        "            patch = patches2_batch[b, i]\n",
        "            q_out = quantum_circuit(patch, weights_l2)\n",
        "            q_out = tf.stack(q_out) if isinstance(q_out, (list, tuple)) else q_out\n",
        "            q_out = tf.cast(tf.math.real(q_out), tf.float32)\n",
        "            q_outputs2.append(tf.reduce_mean(q_out))\n",
        "        q_outputs2 = tf.stack(q_outputs2)\n",
        "        q_outputs2_reshaped = tf.reshape(q_outputs2, (7, 7))\n",
        "        q_outputs2_batch.append(q_outputs2_reshaped)\n",
        "\n",
        "    q_outputs2_batch = tf.stack(q_outputs2_batch)  # (batch_size, 7, 7)\n",
        "    print(\"q_outputs2_batch shape:\", q_outputs2_batch.shape)\n",
        "\n",
        "    # Third layer patches\n",
        "    patches3_batch = extract_patches_batch(q_outputs2_batch, patch_size)  # (batch_size, 9, 4)\n",
        "    print(\"patches3_batch shape:\", patches3_batch.shape)\n",
        "\n",
        "    q_outputs3_batch = []\n",
        "    for b in range(batch_size):\n",
        "        q_outputs3 = []\n",
        "        for i in range(patches3_batch.shape[1]):\n",
        "            patch = patches3_batch[b, i]\n",
        "            q_out = quantum_circuit(patch, weights_l3)\n",
        "            q_out = tf.stack(q_out) if isinstance(q_out, (list, tuple)) else q_out\n",
        "            q_out = tf.cast(tf.math.real(q_out), tf.float32)\n",
        "            q_outputs3.append(tf.reduce_mean(q_out))\n",
        "        q_outputs3 = tf.stack(q_outputs3)\n",
        "        q_outputs3_reshaped = tf.reshape(q_outputs3, (1, -1))  # (1, 9)\n",
        "        q_outputs3_batch.append(q_outputs3_reshaped)\n",
        "\n",
        "    q_outputs3_batch = tf.concat(q_outputs3_batch, axis=0)  # (batch_size, 9)\n",
        "    print(\"q_outputs3_batch shape:\", q_outputs3_batch.shape)\n",
        "\n",
        "    # Pass batch through classical model\n",
        "    logits = classical_model(q_outputs3_batch)  # (batch_size, num_classes)\n",
        "    print(\"logits shape:\", logits.shape)\n",
        "\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ”¹ Training Step with Gradient Tape**\n",
        "\n",
        "- Wraps the forward pass using `tf.GradientTape()` to compute gradients.\n",
        "\n",
        "- Reshapes the label to match the modelâ€™s output.\n",
        "\n",
        "- Computes the loss using the predefined loss function.\n",
        "\n",
        "- Collects **trainable variables**: `weights_l1`, `weights_l2`, and classical model weights.\n",
        "\n",
        "- Calculates gradients and applies them using the Adam optimizer.\n",
        "\n",
        "- Returns both the loss and model predictions.\n"
      ],
      "metadata": {
        "id": "9XORT6CZqUbi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CfB5BUEQHM2"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step_batch(x_imgs, y_labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = forward_batch(x_imgs)  # (batch_size, num_classes)\n",
        "        loss = loss_fn(y_labels, predictions)\n",
        "        tf.print(\"Loss:\", loss)\n",
        "\n",
        "    trainable_vars = [weights_l1, weights_l2, weights_l3] + classical_model.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "    return loss, predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ”¹ Training Loop (Single Image, 20 Epochs)**\n",
        "\n",
        "- Runs for 20 epochs on a single image and label.\n",
        "\n",
        "- In each epoch:\n",
        "  - Performs forward + backward pass using `train_step()`.\n",
        "  - Calculates and prints loss and accuracy.\n",
        "  - Logs detailed output:\n",
        "    - Prediction shape\n",
        "    - Prediction values\n",
        "    - Ground truth label\n",
        "    - Final predicted class\n",
        "\n",
        "- This loop helps track how the model evolves while training on a very small input set.\n"
      ],
      "metadata": {
        "id": "onTBoNiGqW5y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmQhG7tbR-P-",
        "outputId": "9911ac73-f27b-4dd9-8a70-1c2a7cc21fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "patches1_batch shape: (16, 196, 4)\n",
            "q_outputs1_batch shape: (16, 14, 14)\n",
            "patches2_batch shape: (16, 49, 4)\n",
            "q_outputs2_batch shape: (16, 7, 7)\n",
            "patches3_batch shape: (16, 9, 4)\n",
            "q_outputs3_batch shape: (16, 9)\n",
            "logits shape: (16, 2)\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    for batch_images, batch_labels in train_dataset:\n",
        "        loss, predictions = train_step_batch(batch_images, batch_labels)\n",
        "    print(f\"Epoch {epoch + 1} completed\")\n",
        "\n",
        "\n",
        "    # print(f\"Epoch {epoch+1:2d}/{epochs} | Loss: {loss.numpy():.6f} | Accuracy: {acc.numpy()[0]:.4f}\")\n",
        "    # print(f\"  Detailed info at epoch {epoch+1}:\")\n",
        "    # print(f\"    Predictions shape: {predictions.shape}\")\n",
        "    # print(f\"    Predictions: {predictions.numpy().flatten()}\")\n",
        "    # print(f\"    True label: {label}\")\n",
        "    # print(f\"    Predicted class: {tf.argmax(predictions, axis=1).numpy()[0]}\")\n",
        "    # print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Testing function\n",
        "# @tf.function\n",
        "# def test_step(x_img, y_label):\n",
        "#     predictions = forward(x_img)  # shape: (1, 10)\n",
        "#     loss = loss_fn(tf.reshape(y_label, (1,)), predictions)\n",
        "#     predicted_class = tf.argmax(predictions, axis=1)[0]\n",
        "#     return loss, predictions, predicted_class\n",
        "\n",
        "# # Test on full test set\n",
        "# def evaluate_model(x_test, y_test, num_samples=None):\n",
        "#     \"\"\"\n",
        "#     Evaluate the model on test data\n",
        "\n",
        "#     Args:\n",
        "#         x_test: Test images (tensor with shape like (10, 28, 28))\n",
        "#         y_test: Test labels (tensor with shape like (10,))\n",
        "#         num_samples: Number of samples to test (None for all)\n",
        "#     \"\"\"\n",
        "#     if num_samples is None:\n",
        "#         num_samples = x_test.shape[0]\n",
        "\n",
        "#     total_loss = 0.0\n",
        "#     correct_predictions = 0\n",
        "#     all_predictions = []\n",
        "#     all_true_labels = []\n",
        "\n",
        "#     print(f\"Testing on {num_samples} samples...\")\n",
        "#     print(\"-\" * 50)\n",
        "\n",
        "#     for i in range(num_samples):\n",
        "#         test_img = x_test[i]  # Shape: (28, 28)\n",
        "#         test_label = y_test[i]\n",
        "\n",
        "#         # Test the model\n",
        "#         loss, predictions, predicted_class = test_step(test_img, tf.constant(test_label))\n",
        "\n",
        "#         # Accumulate metrics\n",
        "#         total_loss += loss.numpy()\n",
        "#         is_correct = (predicted_class.numpy() == test_label)\n",
        "#         if is_correct:\n",
        "#             correct_predictions += 1\n",
        "\n",
        "#         all_predictions.append(predicted_class.numpy())\n",
        "#         all_true_labels.append(test_label)\n",
        "\n",
        "#         # Print progress every 5 samples (since we only have 10)\n",
        "#         if (i + 1) % 5 == 0 or i == 0:\n",
        "#             print(f\"Sample {i+1:2d} | True: {test_label} | Pred: {predicted_class.numpy()} | \"\n",
        "#                   f\"Correct: {'âœ“' if is_correct else 'âœ—'} | Loss: {loss.numpy():.4f}\")\n",
        "\n",
        "#     # Calculate final metrics\n",
        "#     avg_loss = total_loss / num_samples\n",
        "#     accuracy = correct_predictions / num_samples\n",
        "\n",
        "#     print(\"-\" * 50)\n",
        "#     print(f\"FINAL RESULTS:\")\n",
        "#     print(f\"Test Accuracy: {accuracy:.4f} ({correct_predictions}/{num_samples})\")\n",
        "#     print(f\"Average Loss: {avg_loss:.6f}\")\n",
        "#     print(\"-\" * 50)\n",
        "\n",
        "#     return accuracy, avg_loss, all_predictions, all_true_labels\n",
        "\n",
        "# # Example usage - test on your 10 samples\n",
        "# print(\"Testing model performance...\")\n",
        "# test_accuracy, test_loss, predictions, true_labels = evaluate_model(test_images, test_labels)\n",
        "\n",
        "# # Detailed analysis\n",
        "# def analyze_results(predictions, true_labels):\n",
        "#     \"\"\"Analyze prediction results in detail\"\"\"\n",
        "#     from collections import Counter\n",
        "\n",
        "#     # Class-wise accuracy\n",
        "#     class_correct = Counter()\n",
        "#     class_total = Counter()\n",
        "\n",
        "#     for pred, true in zip(predictions, true_labels):\n",
        "#         class_total[true] += 1\n",
        "#         if pred == true:\n",
        "#             class_correct[true] += 1\n",
        "\n",
        "#     print(\"\\nClass-wise Accuracy:\")\n",
        "#     print(\"-\" * 30)\n",
        "#     for class_id in sorted(class_total.keys()):\n",
        "#         acc = class_correct[class_id] / class_total[class_id] if class_total[class_id] > 0 else 0\n",
        "#         print(f\"Class {class_id}: {acc:.4f} ({class_correct[class_id]}/{class_total[class_id]})\")\n",
        "\n",
        "#     # Confusion matrix (simplified)\n",
        "#     print(f\"\\nMost confused classes:\")\n",
        "#     errors = [(true, pred) for pred, true in zip(predictions, true_labels) if pred != true]\n",
        "#     error_counts = Counter(errors)\n",
        "#     for (true, pred), count in error_counts.most_common(5):\n",
        "#         print(f\"True: {true} â†’ Predicted: {pred} (happened {count} times)\")\n",
        "\n",
        "# # Run detailed analysis\n",
        "# analyze_results(predictions, true_labels)"
      ],
      "metadata": {
        "id": "kBTWUYpip_sU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}